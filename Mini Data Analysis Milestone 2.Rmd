---
title: "Mini Data Analysis Milestone 2"
author: "April Hwang"
output: 
  html_document:
    toc: true
    toc_float: true
    df_print: paged
    theme: cosmo
    highlight: tango
---

###knit to an `.md` file by changing the output in the YAML header to `github_document`, before submitting a tagged release on canvas.*
###submit a link to your tagged release on canvas.

# Introduction

This is the Mini Data-Analysis Deliverable 2 for STATS 545A by April Hwang. It demonstrates my knowledge in making summary tables/graphs, manipulating data, model fitting, and reading/writing data as separate files.

### load library, control message flow

```{r, message = FALSE}
library(tidyverse) 
library(datateachr) 
library(broom) 
library(here)

knitr::opts_chunk$set(message = FALSE) #hide messages
knitr::opts_chunk$set(warning = FALSE) #hide warnings
```

<br>

# Task 1: Process and summarize data 

### 1.1 (1 point): Write research questions from milestone 1

**Research question 1** : What, if any, is the relationship between time and allowance of pet ownership in this dataset? Similarly, is there any relationship between time and availability of accessible parking?

**Research question 2** : Is there a relationship between the number of units and the number of storeys? 

**Research question 3** : What are the variables for which information is most readily available (i.e. no missing data) and least readily available (i.e. large amount of missing data)?

**Research question 4** : What are the top three most popular building amenities? 

<br>

### 1.2 (8 points) For each research question, choose one summarising task and one graphing task 

<br>

###Ensure that the output of each operation is printed!

#### **Research question 1** : What, if any, is the relationship between time and allowance of pet ownership in this dataset? Similarly, is there any relationship between time and availability of accessible parking?

<br>

##### **Summarizing**: Create a categorical variable with 3 or more groups from an existing numerical variable.

I will create a new categorical variable, _year_level_, which identifies the century in which the building was built as _19th century_, _20th century_, and _21st century_.

```{r}
#create year_level and move it forward as first column
apt_buildings %>%
  mutate(year_level = case_when(year_built < 1901 ~ '19th century',
                                year_built < 2001 ~ '20th century', 
                                TRUE ~ '21st century')) %>%
  relocate(year_level)
```

<br>

##### **Graphing**: Create a graph that has at least two geom layers.

I will create a graph using the above _year_level_ on the x-axis and _no_of_accessible_parking_spaces_ on the y-axis. I will use `geom_boxplot()` and `geom_jitter()`. I will also remove a few extreme outliers that contain potentially questionable values of having over 150 accessible parking spaces.

```{r}
#create graph using geom_boxplot() and geom_jitter()
apt_buildings %>%
  mutate(year_level = case_when(year_built < 1901 ~ '19th century',
                                year_built < 2001 ~ '20th century', 
                                TRUE ~ '21st century')) %>%
  ggplot(aes(x = year_level, y = no_of_accessible_parking_spaces)) + 
  geom_boxplot(colour = 'grey') +
  geom_jitter(alpha = 0.2, colour = 'steelblue') + 
  scale_y_log10('no_of_accessible_parking_spaces') +
  theme_classic()
```

<br>
<br>

#### **Research question 2** : Is there a relationship between the number of units and the number of storeys?

<br>

##### **Summarizing**: Create a categorical variable with 3 or more groups from an existing numerical variable.

I will create a new categorical variable, _rise_level_, which classifies each building as _low-rise_, _mid-rise_ or _high-rise_. For this exercise, low-rise buildings will be defined as buildings with 10 floors or under. Mid-rise buildings are defined as buildings that have between 11 to 15 floors. High-rise buildings are defined as buildings that have 16 floors or above.  

```{r}
#create rise_level and move it forward as first column
apt_buildings %>%
  mutate(rise_level = case_when(no_of_storeys < 11 ~ 'low-rise',
                                no_of_storeys < 16 ~ 'mid-rise', 
                                TRUE ~ 'high-rise')) %>%
  relocate(rise_level)
```

<br>

##### **Graphing**: Create a graph that has at least two geom layers. 

I will create a graph using `geom_jitter()` and `geom_smooth()`, incorporating the variable created above, _rise_level_. I will remove one outlier which is listed as having over 4000 units, which may be an error.


```{r}
#create graph using geom_jitter() and geom_smooth()
apt_buildings %>%
  mutate(rise_level = case_when(no_of_storeys < 11 ~ 'low-rise',
                                no_of_storeys < 16 ~ 'mid-rise', 
                                TRUE ~ 'high-rise')) %>%
  ggplot(aes(x = no_of_units, y = no_of_storeys, colour = rise_level)) + 
  geom_jitter(outlier.shape = NA, alpha = 0.2) +
  coord_cartesian(xlim =  c(0, 700)) +
  geom_smooth() +
  theme_classic()
```

<br>
<br>

#### **Research question 3** : What are the variables for which information is most readily available (i.e. no missing data) and least readily available (i.e. large amount of missing data)?

<br>

##### **Summarizing**: Compute the number of observations for at least one of your categorical variables.

For this task, I will compute the number of NAs for all variables in `apt_buildings` and generate a new tibble with the new observations. I will also count the total number of levels as a reference.

```{r}
#count NA and tidy data
apt_buildings_NA <- apt_buildings %>%
  summarise(across(everything(), ~ sum(is.na(.)))) %>% 
  pivot_longer(cols = everything(), names_to = 'col_name', values_to = 'NA_count') 

print(apt_buildings_NA)

#convert apt_building_NA to fct and count levels
as.factor(apt_buildings_NA$col_name) %>%
  nlevels()
```

<br>

##### **Graphing**: Create a graph of your choosing, make one of the axes logarithmic, and format the axes labels so that they are "pretty" or easier to read.

I will create a bar graph using `geom_col` which shows five columns with the highest NA counts. The y-axis will be logarithmic. It will be arranged in a descending order and will include clean and non-overlapping x- and y-axis labels as well as a title for the graph. 

```{r}
#count NA and  tidy data
apt_buildings_NA <- apt_buildings %>%
  summarise(across(everything(), ~ sum(is.na(.)))) %>% 
  pivot_longer(cols = everything(), names_to = 'col_name', values_to = 'NA_count') %>%
  arrange(desc(NA_count))

#select 5 columns with highest NA counts
apt_buildings_NA %>%
  slice_max(NA_count, n = 5) %>%

  #shorten column names
  mutate(col_name = fct_recode(col_name, 
                               'prop_management' = 'prop_management_company_name',
                               'accessible_parking' = 'no_of_accessible_parking_spaces',
                               'barrier_free_accessible_units' = 'no_barrier_free_accessible_units')) %>%

  #plot graph
  ggplot(aes(x = fct_rev(fct_reorder(col_name, NA_count)), y = NA_count)) +
  geom_col(fill = 'steelblue') +
  scale_x_discrete(guide = guide_axis(n.dodge=2)) +
  ggtitle('Columns with Highest NA Count') +
  xlab('Column') + 
  ylab('NA count') +
  theme_minimal()
```

<br>
<br>

#### **Research question 4** : What are the top three most popular building amenities? 

<br>

##### **Summary**: Compute the number of observations for at least one of your categorical variables.

I will convert the _amenities_ variable to a factor, then compute the counts of each level. Then I will tidy the data by converting each cell into a single value, discarding irrelevant columns and omitting NAs. For this research question, popularity of the amenities will be quantified by its frequency.

```{r}
#convert amenities to factor
amenities_fct <- as.factor(apt_buildings$amenities) 

#compute counts of each level
amenities_count <- fct_count(amenities_fct) 

#convert each cell into single value and tidy data
amenities_count %>%
  separate(f, into = c('f1','f2','f3', 'f4','f5','f6'), sep = ' , ') %>% 
  pivot_longer(c('f1','f2','f3', 'f4','f5','f6'), names_to = 'f', values_to = 'amenities_type') %>% 
  select(amenities_type) %>% 
  na.omit() %>% 
  count(amenities_type) #count each amenity type

```

<br>

##### **Graphing**: Create a graph of your choosing, make one of the axes logarithmic, and format the axes labels so that they are "pretty" or easier to read.

I will create a bar graph using `geom_bar` which shows the frequency of each type of amenities using the new tibble from the above summary task. The y-axis will be logarithmic. It will be arranged in an ascending order and will include clean and non-overlapping x- and y-axis labels as well as a title for the graph. 


```{r}
#convert amenities to factor
amenities_fct <- as.factor(apt_buildings$amenities)

#compute counts of each level
amenities_count <- fct_count(amenities_fct) 

#convert each cell into single value and tidy data
amenities_count %>%
  separate(f, into = c('f1','f2','f3', 'f4','f5','f6'), sep = ' , ') %>% 
  pivot_longer(c('f1','f2','f3', 'f4','f5','f6'), names_to = 'f', values_to = 'amenities_type') %>%
  select(amenities_type) %>%
  na.omit() %>%
  
  #plot graph
  ggplot(aes(x = fct_rev(fct_infreq(amenities_type)))) +
  geom_bar(fill = 'steelblue', width = 0.9) +
  scale_x_discrete(guide = guide_axis(n.dodge=2)) +
  ggtitle('Amenity Type Frequency') +
  xlab('Type') + 
  ylab('Frequency') +
  theme_minimal()


```

<br>
<br>

### 1.3 (2 points) Based on above operations, how much closer are you to answering your research questions? 

<br>

**Research question 1** : What, if any, is the relationship between time and allowance of pet ownership in this dataset? Similarly, is there any relationship between time and availability of accessible parking?

_I now have more information to answer the second half of this research question regarding the relationship between time and availability of accessible parking. It appears that there is a disproportionately large quantity of data for buildings that were built in the 20th century, thus this question can be further refined to examine data from within the 20th century. The relationship between time and allowance of pet ownership still remains unclear and it would be better if this research question was separated into two distinct questions._

<br>

**Research question 2** : Is there a relationship between the number of units and the number of storeys? 

_The above operations demonstrate that there is a positive correlation between the number of units and the number of storeys. This is interesting as it aligns with my hypothesis and suggests that buildings with larger units are being built higher rather than wider._

<br>

**Research question 3** : What are the variables for which information is most readily available (i.e. no missing data) and least readily available (i.e. large amount of missing data)?

_The above operations showed that some variables, such as id and bike_parking, do not contain any missing data, whereas other variables, such as amenities and prop_management_company_name, contain a high number of missing data. The results of this question is of interest as the variation in NA count for the different variables is noticeably large. Furthermore, this research question is of a personal interest as filtering and navigating through missing data is an important aspect of my personal research._ 

<br>

**Research question 4** : What are the top three most popular building amenities? 

_According to this dataset, the top three most popular building amenities are outdoor recreation facilities, indoor recreation room and indoor exercise room._

<br>
<br>

# Task 2: Tidy your data 

### 2.1 (2 points) Identify if your data is tidy or untidy.

`apt_buildings` is untidy for the following reasons:

* _amenities_ contains multiple measurements 
* _bike_parking_ contains multiple measurements 
* _parking_type_ contains multiple measurements 
* _facilities_available_ contains multiple measurements 

_apt_buildings_ does contain some aspects of a tidy dataset in that each variable is a column and each column is a variable, and each observation is a row and each row is an observation.

<br>

### 2.2 (4 points) If your data is untidy, then tidy it! Then, untidy it back to it's original state.

In section 1.2, I demonstrated tidying the data for _amenities_ and _NA_count_. For this task, I will tidy _facilities_available_. One of the values for this variable contains multiple measurements (_'Green Bin / Organics'_) and it also reports missing data with two separate labels (_'Unknown'_ and _'Not Available'_).

<br>

**Before (untidy):**
```{r}
#convert chr to fct
apt_buildings_fct <- apt_buildings %>% mutate(across(where(is_character),as_factor))

#identify levels which are untidy
levels(apt_buildings_fct$facilities_available) 
```

<br>

**After (tidy then untidy):**
```{r}
#convert chr to fct
apt_buildings_fct <- apt_buildings %>% mutate(across(where(is_character),as_factor))

#tidy data
apt_buildings_tidy <- apt_buildings_fct %>%
    mutate(facilities_available = fct_recode(facilities_available, 
                                           'Compost' = 'Green Bin / Organics',
                                           'NA' = 'Unknown',
                                           'NA.' = 'Not Available'))

#check that data is tidy
print(select(apt_buildings_tidy,facilities_available))


#untidy data
apt_buildings_untidy <- apt_buildings_tidy %>%
    mutate(facilities_available = fct_recode(facilities_available, 
                                           'Green Bin / Organics' = 'Compost',
                                           'Unknown' = 'NA',
                                           'Not available' = 'NA.'))

#check that data is untidy
print(select(apt_buildings_untidy,facilities_available))
```

<br>
<br>

### 2.3 (4 points) Pick 2 of the 4 research questions to continue your analysis.

<br>

1. Is there a relationship between the number of units and the number of storeys?

2.  What are the variables for which information is most readily available (i.e. no missing data) and least readily available (i.e. large amount of missing data)?

<br>

I chose the above two questions as the first choice demonstrated promising results of which confirms my hypothesis of positive correlation between the number of units and the number of storeys. It will be interesting to continue with the analysis to explore the robustness of this correlation. The second choice is of a personal interest as managing missing data is an important part of my personal research and I would like to gain more experience in working with missing data.


<!--------------------------- Start your work below --------------------------->

<!----------------------------------------------------------------------------->

Now, try to choose a version of your data that you think will be appropriate to answer these 2 questions. Use between 4 and 8 functions that we've covered so far (i.e. by filtering, cleaning, tidy'ing, dropping irrelevant columns, etc.).

(If it makes more sense, then you can make/pick two versions of your data, one for each research question.) 

<!--------------------------- Start your work below --------------------------->

# Task 3: Modelling

## 3.0 (no points)

Pick a research question from 1.2, and pick a variable of interest (we'll call it "Y") that's relevant to the research question. Indicate these.

<!-------------------------- Start your work below ---------------------------->

**Research Question**: Is there a relationship between the number of units and the number of storeys?

**Variable of interest**: Number of units

<!----------------------------------------------------------------------------->

## 3.1 (3 points)

Fit a model or run a hypothesis test that provides insight on this variable with respect to the research question. Store the model object as a variable, and print its output to screen. We'll omit having to justify your choice, because we don't expect you to know about model specifics in STAT 545.

Hypothesis: There will be a positive correlation between the number of storeys and the number of units.

I will fit a model that makes predictions on the number of units using the number of storeys by using the `lm()` function.


<!-------------------------- Start your work below ---------------------------->

```{r}

#prep data using 4-8 functions
apt_buildings_unit_storeys <- apt_buildings %>%
  
  #use select() to drop irrelevant columns
  select(no_of_storeys, no_of_units) %>% 
  
  #use arrange() to reorder rows 
  relocate(no_of_units) %>%
  
  #use desc() to sort rows in descending order
  arrange(desc(no_of_units)) %>%

  #use rename() to shorten column labels
  rename(storeys = no_of_storeys,
         units = no_of_units) %>%
  
  #use slice() to remove first outlier row with 4000+ units
  slice(-1)

print(apt_buildings_unit_storeys)

#linear regression modeling

apt_buildings_unit_storeys %>%
  ggplot(aes(x = storeys, y = units)) +
    geom_point(colour = 'steelblue') +
    geom_smooth(method = 'lm', colour = 'black') +
    theme_minimal()

```


<!----------------------------------------------------------------------------->

## 3.2 (3 points)

Produce something relevant from your fitted model: either predictions on Y, or a single value like a regression coefficient or a p-value.

-   Be sure to indicate in writing what you chose to produce.
-   Your code should either output a tibble (in which case you should indicate the column that contains the thing you're looking for), or the thing you're looking for itself.
-   Obtain your results using the `broom` package if possible. If your model is not compatible with the broom function you're needing, then you can obtain your results by some other means, but first indicate which broom function is not compatible.

<!-------------------------- Start your work below ---------------------------->

For this task, I will produce the p-value by using broom::glance().

```{r}
apt_buildings_lm <- lm(no_of_units ~ no_of_storeys, data = apt_buildings)

#use glance() to calculate p-value
glance(apt_buildings_lm) %>%
  
  #indicate p.value column
  select(p.value)

```


<!----------------------------------------------------------------------------->

# Task 4: Reading and writing data

Get set up for this exercise by making a folder called `output` in the top level of your project folder / repository. You'll be saving things there.

## 4.1 (3 points)

Take a summary table that you made from Task 1, and write it as a csv file in your `output` folder. Use the `here::here()` function.

-   **Robustness criteria**: You should be able to move your Mini Project repository / project folder to some other location on your computer, or move this very Rmd file to another location within your project repository / folder, and your code should still work.
-   **Reproducibility criteria**: You should be able to delete the csv file, and remake it simply by knitting this Rmd file.

<!-------------------------- Start your work below ---------------------------->
```{r}

#write summary table as csv file using here()
write_csv(apt_buildings_NA, here('output', 'apt_buildings_NA.csv'))


```

<!----------------------------------------------------------------------------->

## 4.2 (3 points)

Write your model object from Task 3 to an R binary file (an RDS), and load it again. Be sure to save the binary file in your `output` folder. Use the functions `saveRDS()` and `readRDS()`.

-   The same robustness and reproducibility criteria as in 4.1 apply here.

<!-------------------------- Start your work below ---------------------------->

```{r}
#use saveRDS() to write model object to RDS file
saveRDS(apt_buildings_lm, file = here('output', 'apt_buildings_lm.rds'))

#use readRDS() to load RDS file
readRDS(here('output', 'apt_buildings_lm.rds'))
```


<!----------------------------------------------------------------------------->

# Overall Reproducibility/Cleanliness/Coherence Checklist 

Here are the criteria we're looking for.

## Coherence (0.5 points)

The document should read sensibly from top to bottom, with no major continuity errors. 

The README file should still satisfy the criteria from the last milestone, i.e. it has been updated to match the changes to the repository made in this milestone. 

## File and folder structure (1 points)

You should have at least three folders in the top level of your repository: one for each milestone, and one output folder. If there are any other folders, these are explained in the main README.

Each milestone document is contained in its respective folder, and nowhere else.

Every level-1 folder (that is, the ones stored in the top level, like "Milestone1" and "output") has a `README` file, explaining in a sentence or two what is in the folder, in plain language (it's enough to say something like "This folder contains the source for Milestone 1").

## Output (1 point)

All output is recent and relevant:

-   All Rmd files have been `knit`ted to their output md files. 
-   All knitted md files are viewable without errors on Github. Examples of errors: Missing plots, "Sorry about that, but we can't show files that are this big right now" messages, error messages from broken R code
-   All of these output files are up-to-date -- that is, they haven't fallen behind after the source (Rmd) files have been updated.
-   There should be no relic output files. For example, if you were knitting an Rmd to html, but then changed the output to be only a markdown file, then the html file is a relic and should be deleted.

Our recommendation: delete all output files, and re-knit each milestone's Rmd file, so that everything is up to date and relevant.

## Tagged release (0.5 point)

You've tagged a release for Milestone 2. 

### Attribution

Thanks to Victor Yuan for mostly putting this together.